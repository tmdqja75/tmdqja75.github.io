---
layout: post
title: PlayData Day 13
description: >
    
# image: /assets/img/pythonLogo.png
sitemap: false
categories:
  - studylog
  - playdata
---

# PlayData AI Bootcamp Day 13

* this unordered seed list will be replaced by the toc
{:toc}

https://app.gather.town/invite?token=jJGNuy26QCim_rA6a_NkWils0LXyTUr2

## 웹사이트의 구조
- DOM (Document Object Model) Tree: HTML Object (tags)들이 어떤 부모/자식 구조로 형성되어 있는지 보여줍니다.

![DOMTree](https://www.w3schools.com/js/pic_htmltree.gif)

Credit: https://www.w3schools.com/js/js_htmldom.asp
{:.figcation}

### HTTP 응답코드
http로 요청했을 떄 상태/오류를 코드로 나타내줍니다.
[응답코드 리스트](https://namu.wiki/w/HTTP/%EC%9D%91%EB%8B%B5%20%EC%BD%94%EB%93%9C)


## 웹 크롤링
- HTML 태그들을 가지고 원하는 데이터가 어디있는지 확인하고 수집할 수 있습니다.
- 데이터를 바로 긁어올 수 없고 HTML등들이 다 불러올 때까지 기다린 다음 불러올 수 있습니다.

![htmlload](/assets/img/PlayDataNote/Day13/htmlloadtime.png)

![htmlload](/assets/img/PlayDataNote/Day13/htmlloadtime_DOM.png)

웹사이트와 관련된 정보들을 불러오는데 약간의 시간이 걸리는 것을 확인할 수 있습니다.
{:.figcation}


### Socket을 이용한 HTTP 통신
- 아래 코드는 socket 패키지로 `http://info.cern.ch` 웹사이트에 있는 정보를 가져옵니다.
```python
import socket

serverAddress = socket.gethostbyname('info.cern.ch')
serverPort = 80

sock = socket.socket( socket.AF_INET, socket.SOCK_STREAM )
sock.connect( (serverAddress, serverPort) )

method = 'GET' 
request_header = f'{method} /index.html HTTP/1.1\r\n'
request_header += 'Host: info.cern.ch\r\n'
request_header += '\r\n'

sock.send( request_header.encode() )
response = sock.recv(1024)
print( response.decode() )

sock.close()
```
```html
HTTP/1.1 200 OK
Date: Thu, 31 Mar 2022 13:15:43 GMT
Server: Apache
Last-Modified: Wed, 05 Feb 2014 16:00:31 GMT
ETag: "286-4f1aadb3105c0"
Accept-Ranges: bytes
Content-Length: 646
Connection: close
Content-Type: text/html

<html><head></head><body><header>
<title>http://info.cern.ch</title>
</header>

<h1>http://info.cern.ch - home of the first website</h1>
<p>From here you can:</p>
<ul>
<li><a href="http://info.cern.ch/hypertext/WWW/TheProject.html">Browse the first website</a></li>
<li><a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html">Browse the first website using the line-mode browser simulator</a></li>
<li><a href="http://home.web.cern.ch/topics/birth-web">Learn about the birth of the web</a></li>
<li><a href="http://home.web.cern.ch/about">Learn about CERN, the physics laboratory where the web was born</a></li>
</ul>
</body></html>
```

### 라이브러리를 이용한 HTTP 통신
- 파이썬 기본 패키지 (`urllib`)로 http 통신 하기
- socket보다 훨씬 간단한 코드로 웹크롤링을 할 수 있습니다.

```python
import urllib.request
url = 'http://info.cern.ch'

# 지정 url에 요청
request = urllib.request.Request(url)
# 응답 response variable에 담기
response = urllib.request.urlopen(request)

# 읽기 쉬운 형태로 print
print(response.read().decode())
```
결과: 
```html
<html><head></head><body><header>
<title>http://info.cern.ch</title>
</header>

<h1>http://info.cern.ch - home of the first website</h1>
<p>From here you can:</p>
<ul>
<li><a href="http://info.cern.ch/hypertext/WWW/TheProject.html">Browse the first website</a></li>
<li><a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html">Browse the first website using the line-mode browser simulator</a></li>
<li><a href="http://home.web.cern.ch/topics/birth-web">Learn about the birth of the web</a></li>
<li><a href="http://home.web.cern.ch/about">Learn about CERN, the physics laboratory where the web was born</a></li>
</ul>
</body></html>
```

#### fake_useragent
- 많은 웹사이트들은 보안을 위해 웹브라우저를 통해 접속하는 것 이외의 접속이 감지되면 그 접속을 막는 경우가 있습니다. 이를 우회하기 위해 python 패키지 중 하나인 `fake_useraget`를 이용하여 웹브리우저에서 접속하는 것처럼 파이썬에서 웹크롤링을 할 수 있습니다.
```python
import urllib.request
from fake_useragent import UserAgent

# 웹브라우저를 통해 요청하는 것처럼 웹사이트를 접속하기 위해 헤더값을 직접 설정해줘야 합니다.
agent = UserAgent()
header = { 'User-Agent': agent.chrome }
url = 'http://info.cern.ch'

#요청
request = urllib.request.Request(url)
#응답
response = urllib.request.urlopen(request)
print(response.read().decode())
```
결과:
```html
<html><head></head><body><header>
<title>http://info.cern.ch</title>
</header>

<h1>http://info.cern.ch - home of the first website</h1>
<p>From here you can:</p>
<ul>
<li><a href="http://info.cern.ch/hypertext/WWW/TheProject.html">Browse the first website</a></li>
<li><a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html">Browse the first website using the line-mode browser simulator</a></li>
<li><a href="http://home.web.cern.ch/topics/birth-web">Learn about the birth of the web</a></li>
<li><a href="http://home.web.cern.ch/about">Learn about CERN, the physics laboratory where the web was born</a></li>
</ul>
</body></html>
```

### request를 통한 HTTP통신
- `request` 패키지를 이용해 더 간단하게 웹사이트에 대한 다양한 정보를 가져올 수 있습니다.

```python
import requests
url='http://info.cern.ch/'

response = requests.get(url)
print(response.text)
```
```html
<html><head></head><body><header>
<title>http://info.cern.ch</title>
</header>

<h1>http://info.cern.ch - home of the first website</h1>
<p>From here you can:</p>
<ul>
<li><a href="http://info.cern.ch/hypertext/WWW/TheProject.html">Browse the first website</a></li>
<li><a href="http://line-mode.cern.ch/www/hypertext/WWW/TheProject.html">Browse the first website using the line-mode browser simulator</a></li>
<li><a href="http://home.web.cern.ch/topics/birth-web">Learn about the birth of the web</a></li>
<li><a href="http://home.web.cern.ch/about">Learn about CERN, the physics laboratory where the web was born</a></li>
</ul>
</body></html>
```



## 스크래핑
- 웹페이지에서 가져온 데이터 중 사용자가 원하는 데이터만 추출해내는 것을 의미합니다.
- 스크래핑을 위한 대표적인 패키지 중 하나는 `Beautiful Soup`이라는 패키지입니다.

### Beautiful Soup
- Beautiful Soup 패키지를 import 할때는 `import bs4` 로 불러옵니다.
- `requests`등과 같은 패키지들로 html을 불러온 뒤 `bs4.BeautifulSoup()` 메소드에 넣어주면 html을 `bs4.BeautifulSoup` 타입으로 저장해 줍니다
- `BeautifulSoup`타입으로 변횐된 html을 Beautiful Soup의 `find()` 또는 `find_all()`메소드로 태그에 달린 원하는 데이터를 찾을 수 있습니다.
- 원하는 데이터가 어떤 태그 아래에 있는지는 웹브라우저의 Developer Tool 에서 찾을 수 있습니다.
- 아래 예시는 BeautifulSoup으로 네이버 영화 웹사이트에서 영화 평점과 리뷰를 가져오는 코드입니다.
```python
import requests
import bs4 # Beautiful Soup 불러오기

# request 패키지로 html 통째로 크롤링하기
url = 'https://movie.naver.com/movie/point/af/list.naver'
response = requests.get(url)
html = response.text

# html-> BeautifulSoup로 변환
review = bs4.BeautifulSoup(html)


att = {'class':'title'} # 태그 아래에 있는 Attributes
# find_all 메소드로 <td> 태그가 있는 데이터 리스트로 받기
elements = review.find_all('td', attrs=att)

for element in elements:
    title = element.text.split('\n')[1]
    score = element.find('em').text
    review = element.text.split('\n')[5]

    print(f'영화제목: {title}, 별점: {score}, 리뷰: {review}')

```
결과:
```
영화제목: 배니싱: 미제사건, 별점: 5, 리뷰: 구성이랑 연출이 따로 논다제작자랑 감독이랑 싸웠나? 
영화제목: 배니싱: 미제사건, 별점: 5, 리뷰: 시작은 좋았는데...뜬금없는 전개와 결말... 
영화제목: 효자, 별점: 10, 리뷰: 
영화제목: 블랙 크랩, 별점: 5, 리뷰: 누미 라파스 연기와 아슬아슬 살얼음판 스케이팅 장면은 볼만하다 
영화제목: 나의 소녀시대, 별점: 9, 리뷰: 그 시절 우리는 같은 남자아이를 좋아했고 같은 여자아이를 좋아했고 그래서 상처받고 그래서 성숙해졌다. 손발이 오글거리지만 어느 순간 가슴으로 확 와닿는 첫사랑의 추억~ 잘 지내는가 그대는 
...
```

